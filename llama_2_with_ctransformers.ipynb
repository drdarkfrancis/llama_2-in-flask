{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Package"
      ],
      "metadata": {
        "id": "nhKRx0RNeOvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "vcztM0BggUoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hu74s0OdmQ3"
      },
      "outputs": [],
      "source": [
        "!pip install ctransformers ctransformers[cuda] -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the model from hf"
      ],
      "metadata": {
        "id": "CsC-prUXrwjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub>=0.17.1"
      ],
      "metadata": {
        "id": "4Gz_nBQnr5Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "id": "bmy7XPwPrH4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Model"
      ],
      "metadata": {
        "id": "d-jpwEUReVSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Load directly from hf* :"
      ],
      "metadata": {
        "id": "RTa1m0-_stiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-chat-GGUF\", gpu_layers=50)"
      ],
      "metadata": {
        "id": "dU_8RrFueeYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Load from the local model* :"
      ],
      "metadata": {
        "id": "6L9mNUzZs3yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Path to the locally saved model\n",
        "model_load_path = \"/content/llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "# Load the model from the local path\n",
        "llm = AutoModelForCausalLM.from_pretrained(model_load_path, gpu_layers=50)"
      ],
      "metadata": {
        "id": "QDD20eaDsR4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"Respond as if you are a character from a Shakespearean play, using archaic language and dramatic flair in your answers.\"\n",
        "user_message = \"describe how black holes are created\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_message} [/INST]\""
      ],
      "metadata": {
        "id": "gVfY2AFt0ezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"you are a professional legal contractor ,answer the question based on the provided context\"\n",
        "user_message = \"Transfer of personal data outside Poland â€¢ The Personal Data Administrator does not transfer personal data outside the European Union. into: o countries of the European Economic Area; o other countries (third countries). â€¢ Cross-border processing of personal data within the EEA is treated as if it were processed within Poland. â€¢ In the case of transfer of personal data to a third country, one of the following conditions must be met: o the destination country guarantees the protection of personal data on its territory at least as well as in force on the territory of the Republic of Poland; o when the transfer of personal data results from an obligation imposed by law or the provisions of a ratified international agreement; o the President of the Office for Personal Data Protection will consent to the transfer of personal data. the question is : is the data transfered outside of europe?\"\n",
        "\n",
        "prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_message} [/INST]\""
      ],
      "metadata": {
        "id": "S6lodwgAnubO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "AiC20Nhg4RGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Text"
      ],
      "metadata": {
        "id": "4HwKRsrfeokq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm(prompt)"
      ],
      "metadata": {
        "id": "SPxxmNb1eptT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "RMaP2Zwop6cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XnfXSWiMi700"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracking GPU usage"
      ],
      "metadata": {
        "id": "vVJ_7JleolTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "def get_gpu_usage():\n",
        "    # Get the current GPU usage using nvidia-smi\n",
        "    result = subprocess.run(['nvidia-smi', '--format=csv', '--query-gpu=utilization.gpu,utilization.memory'],\n",
        "                            stdout=subprocess.PIPE)\n",
        "    return result.stdout.decode('utf-8')"
      ],
      "metadata": {
        "id": "tsOXRPC_oVkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform inference\n",
        "response = llm(prompt)\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the inference time\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference Time: {inference_time} seconds\")\n",
        "\n",
        "# Print GPU usage\n",
        "print(\"GPU Usage during Inference:\")\n",
        "print(get_gpu_usage())"
      ],
      "metadata": {
        "id": "BQlasGsVoYWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}